{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install PyTorch and related libraries ","metadata":{}},{"cell_type":"code","source":"!pip install torch torchvision torchaudio","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  Setup and Imports\n\n- Sets a CUDA memory config (`expandable_segments:True`) to prevent out-of-memory errors during dynamic allocation in PyTorch.\n- Imports essential libraries for:\n  -  **Audio processing**: `librosa`, `torchaudio`, `soundfile`\n  -  **Visualization**: `matplotlib`, `librosa.display`\n  -  **Deep learning**: `torch`, `torch.nn`, `torch.optim`, `amp` for mixed-precision training\n  -  **Model evaluation**: `mean_squared_error`, `cosine` distance\n  -  **Data management**: `Dataset`, `DataLoader`, `train_test_split`\n  -  **Others**: `tqdm` for progress bars, `PIL` for image operations, `warnings` to suppress logs\n","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\nimport math\nimport librosa\nimport librosa.display\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.cuda.amp as amp\nimport soundfile as sf\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.spatial.distance import cosine\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.auto import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ChebyshevKANLinear Layer","metadata":{}},{"cell_type":"code","source":"# ChebyshevKANLinear class with normalization\nclass ChebyshevKANLinear(nn.Module):\n    def __init__(self, in_features, out_features, degree=5, scale_base=1.0, scale_cheb=1.0, device='cuda'):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.degree = degree\n        self.device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n\n        self.base_weight = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.cheb_weight = nn.Parameter(torch.Tensor(out_features, in_features, degree + 1))\n        self.scale_base = scale_base\n        self.scale_cheb = scale_cheb\n        self.layernorm = nn.LayerNorm(in_features).to(self.device)  # Added normalization\n\n        self.base_activation = nn.PReLU().to(self.device)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.base_weight, gain=0.1)  # Reduced gain\n        nn.init.xavier_uniform_(self.cheb_weight, gain=0.1)  # Reduced gain\n\n    def chebyshev_polynomials(self, x):\n        x = x.to(self.device)\n        T = [torch.ones_like(x), x]\n        for n in range(2, self.degree + 1):\n            Tn = 2 * x * T[-1] - T[-2]\n            T.append(Tn.clamp(-10, 10))  # Clamp to prevent explosion\n        return torch.stack(T, dim=-1)  # (batch, in_features, degree+1)\n\n    def forward(self, x):\n        x = self.layernorm(x)  # Normalize input to [-1, 1] range\n        x = x.clamp(-1, 1)  # Additional clamping for safety\n        x = x.to(self.device)\n        base_out = F.linear(self.base_activation(x), self.base_weight)\n        cheb_terms = self.chebyshev_polynomials(x)  # (batch, in_features, degree+1)\n        cheb_proj = torch.einsum('bid,oif->bo', cheb_terms, self.cheb_weight)\n        output = self.scale_base * base_out + self.scale_cheb * cheb_proj\n        if torch.isnan(output).any():\n            print(\"Warning: NaN detected in ChebyshevKANLinear output\")\n        return output\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# AudioProcessor: Audio Feature Utility Class\n\nThis helper class handles key audio preprocessing and visualization tasks.\n\n**Core Methods:**\n- `mel_spectrogram_to_audio`:  \n  ðŸ” Converts a Mel spectrogram (in dB) back into waveform audio using:\n  - `librosa.db_to_power()` â†’ converts dB to power\n  - `mel_to_stft()` â†’ approximates original STFT\n  - `griffinlim()` â†’ reconstructs waveform via iterative phase estimation\n\n- `plot_mel_spectrogram`:  \n  ðŸ“Š Plots the given Mel spectrogram using `librosa.display.specshow()`  \n  - Adds axes, color bar, and title for easy interpretation","metadata":{}},{"cell_type":"code","source":"# AudioProcessor class\nclass AudioProcessor:\n    def __init__(self, sr=22050, n_mels=128, n_fft=2048, hop_length=512):\n        self.sr = sr\n        self.n_mels = n_mels\n        self.n_fft = n_fft\n        self.hop_length = hop_length\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    def mel_spectrogram_to_audio(self, mel_spec_db):\n        mel_spec = librosa.db_to_power(mel_spec_db)\n        stft = librosa.feature.inverse.mel_to_stft(mel_spec, sr=self.sr, n_fft=self.n_fft)\n        audio = librosa.griffinlim(stft, hop_length=self.hop_length)\n        return audio\n\n    def plot_mel_spectrogram(self, mel_spec_db, title=\"Mel Spectrogram\"):\n        plt.figure(figsize=(12, 6))\n        librosa.display.specshow(\n            mel_spec_db, sr=self.sr, hop_length=self.hop_length, x_axis='time', y_axis='mel'\n        )\n        plt.colorbar(format='%+2.0f dB')\n        plt.title(title)\n        plt.tight_layout()\n        plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# AudioDataset: PyTorch Dataset Wrapper for Mel Spectrograms","metadata":{}},{"cell_type":"code","source":"# AudioDataset class\nclass AudioDataset(Dataset):\n    def __init__(self, mel_spectrograms):\n        self.mel_spectrograms = mel_spectrograms\n\n    def __len__(self):\n        return len(self.mel_spectrograms)\n\n    def __getitem__(self, idx):\n        return torch.FloatTensor(self.mel_spectrograms[idx])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ImprovedAudioVAE: Audio VAE with ChebyshevKANLinear_Layers\n\nThis version of `ImprovedAudioVAE` enhances the encoder with ChebyshevKANLinear\n---\n\n###  Initialization (`__init__`)\n- Initializes VAE parameters and builds:\n  - `Encoder`\n  - `Bottleneck` with ChebyshevKANLinear\n  - `Decoder` using transposed convolutions","metadata":{}},{"cell_type":"code","source":"class ImprovedAudioVAE(nn.Module):\n    def __init__(self, input_shape, conv_filters=(16, 32, 64, 256), conv_kernels=(3, 3, 3, 3),\n                 conv_strides=(1, 2, 2, 2), latent_dim=256, dropout_rate=0.3):\n        super(ImprovedAudioVAE, self).__init__()\n        self.input_shape = input_shape\n        self.conv_filters = conv_filters\n        self.conv_kernels = conv_kernels\n        self.conv_strides = conv_strides\n        self.latent_dim = latent_dim\n        self.dropout_rate = dropout_rate\n        self.num_conv_layers = len(conv_filters)\n        self.shape_before_bottleneck = None\n\n        self.encoder = self._build_encoder()\n        self.decoder = self._build_decoder()\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n            torch.nn.init.xavier_uniform_(module.weight)\n            if module.bias is not None:\n                torch.nn.init.constant_(module.bias, 0)\n        elif isinstance(module, KANLinear):\n            nn.init.kaiming_uniform_(module.wavelet_weights, a=math.sqrt(5))\n            nn.init.kaiming_uniform_(module.weight1, a=math.sqrt(5))\n        elif isinstance(module, ChebyshevKANLinear):\n            module.reset_parameters()\n\n    def _build_encoder(self):\n        layers = []\n        in_channels = 1\n        for i, (filters, kernel, stride) in enumerate(zip(self.conv_filters, self.conv_kernels, self.conv_strides)):\n            layers.extend([\n                nn.Conv2d(\n                    in_channels=in_channels,\n                    out_channels=filters,\n                    kernel_size=kernel,\n                    stride=stride,\n                    padding=kernel // 2,\n                ),\n                nn.ReLU(),\n                nn.BatchNorm2d(filters),\n                nn.Dropout(self.dropout_rate)\n            ])\n            in_channels = filters\n\n        with torch.no_grad():\n            dummy_input = torch.zeros(1, 1, *self.input_shape)\n            x = dummy_input\n            for layer in layers:\n                x = layer(x)\n            self.shape_before_bottleneck = x.shape[1:]\n\n        flat_dim = np.prod(self.shape_before_bottleneck)\n        layers.append(nn.Flatten())\n        self.mu = ChebyshevKANLinear(flat_dim, self.latent_dim, degree=3, scale_base=1.0, scale_cheb=1.0)\n        self.logvar = ChebyshevKANLinear(flat_dim, self.latent_dim, degree=3, scale_base=1.0, scale_cheb=1.0)\n        return nn.Sequential(*layers)\n\n    def _build_decoder(self):\n        layers = []\n        num_neurons = np.prod(self.shape_before_bottleneck)\n        layers.extend([\n            nn.Linear(self.latent_dim, num_neurons),\n            nn.ReLU(),\n            nn.Unflatten(1, self.shape_before_bottleneck)\n        ])\n\n        in_channels = self.conv_filters[-1]\n        for i in reversed(range(1, self.num_conv_layers)):\n            layers.extend([\n                nn.ConvTranspose2d(\n                    in_channels=in_channels,\n                    out_channels=self.conv_filters[i-1],\n                    kernel_size=self.conv_kernels[i],\n                    stride=self.conv_strides[i],\n                    padding=self.conv_kernels[i] // 2,\n                    output_padding=0\n                ),\n                nn.ReLU(),\n                nn.BatchNorm2d(self.conv_filters[i-1]),\n                nn.Dropout(self.dropout_rate)\n            ])\n            in_channels = self.conv_filters[i-1]\n\n        layers.extend([\n            nn.ConvTranspose2d(\n                in_channels=in_channels,\n                out_channels=1,\n                kernel_size=self.conv_kernels[0],\n                stride=self.conv_strides[0],\n                padding=self.conv_kernels[0] // 2,\n                output_padding=0\n            ),\n            nn.Sigmoid()\n        ])\n        return nn.Sequential(*layers)\n\n    def encode(self, x):\n        if x.dim() == 3:\n            x = x.unsqueeze(1)\n        elif x.dim() == 2:\n            x = x.unsqueeze(0).unsqueeze(0)\n        h = checkpoint.checkpoint_sequential(self.encoder, len(self.encoder), x)\n        h = torch.tanh(h)\n        mu = self.mu(h)\n        logvar = self.logvar(h)\n        return mu, logvar\n\n    def decode(self, z):\n        x = checkpoint.checkpoint_sequential(self.decoder, len(self.decoder), z)\n        if x.dim() == 3:\n            x = x.unsqueeze(1)\n        recon_x = torch.nn.functional.interpolate(x, size=self.input_shape, mode='bilinear', align_corners=False)\n        return recon_x\n\n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def forward(self, x):\n        if x.dim() == 3:\n            x = x.unsqueeze(1)\n        elif x.dim() == 2:\n            x = x.unsqueeze(0).unsqueeze(0)\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        recon_x = self.decode(z)\n        return recon_x, mu, logvar","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  AudioDeepfakeGenerator: End-to-End Deepfake Audio Generator Using VAE\n\n\nThis class orchestrates the full workflow for generating deepfake audio using a **Variational Autoencoder (VAE)** trained on Mel spectrograms.\n\n---\n\n### Initialization (`__init__`)\n- Sets up:\n  - `AudioProcessor` (Mel spectrogram utils)\n  - VAE model placeholder\n  - `GradScaler` for mixed-precision training\n- Target input shape defaults to 128x128\n\n---\n\n### Preprocessing (`preprocess_mel_spectrograms`)\n- Pads or crops Mel spectrograms to target shape\n- Normalizes values between 0â€“1 using robust min-max scaling\n- Converts valid spectrograms into a 4D NumPy array for training\n\n---\n\n### VAE Training (`train_vae`)\n- Initializes and trains an `ImprovedAudioVAE` model:\n  - Customizable epochs, learning rate, batch size, KL-Î² weight\n  - Uses gradient accumulation + mixed precision\n  - Saves best + periodic model checkpoints\n  - Applies early stopping based on validation loss\n\n---\n\n### VAE Loss (`improved_vae_loss`)\n- Combines:\n  - MSE reconstruction loss\n  - KL divergence loss (scaled by `Î²`)\n\n---\n\n### Visualization (`plot_training_curve`)\n- Plots training loss curve over epochs\n\n---\n\n### Deepfake Generation (`generate_deepfake`)\n- Uses a trained VAE to reconstruct + modify original Mel spectrograms\n- Adds controllable Gaussian noise in latent space\n- Returns reconstructed Mel spectrograms for each noise level\n\n---\n\n### Evaluation (`evaluate_model`)\n- Computes average loss on a test set using same VAE loss\n- Useful for monitoring model generalization\n\n---\n\n### Comparison (`compare_audio`)\n- Compares original and fake Mel spectrograms:\n  - MSE (Mean Squared Error)\n  - Cosine similarity\n  - Pearson correlation\n\n---\n\n### Visualization (`visualize_comparison`)\n- Plots:\n  - Original Mel\n  - Generated Mel\n  - Absolute difference between them\n\n---\n\n### Audio Output (`save_audio`)\n- Converts a Mel spectrogram back to waveform\n- Saves output using `soundfile.write`\n\n---\n\n","metadata":{}},{"cell_type":"code","source":"class AudioDeepfakeGenerator:\n    def __init__(self, height=128, width=128):\n        self.processor = AudioProcessor(n_mels=height, sr=22050, n_fft=2048, hop_length=512)\n        self.vae = None\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        print(f\"Using device: {self.device}\")\n        self.target_shape = (height, width)\n\n    def preprocess_mel_spectrograms(self, file_data):\n        mel_spectrograms = []\n        successful_files = 0\n        print(f\"Processing {len(file_data)} mel spectrogram image files...\")\n\n        for file_path, mel_spec in file_data:\n            try:\n                if mel_spec.shape[0] > self.target_shape[0]:\n                    mel_spec = mel_spec[:self.target_shape[0], :]\n                elif mel_spec.shape[0] < self.target_shape[0]:\n                    pad_height = self.target_shape[0] - mel_spec.shape[0]\n                    mel_spec = np.pad(mel_spec, ((0, pad_height), (0, 0)), 'constant', constant_values=mel_spec.min())\n\n                if mel_spec.shape[1] > self.target_shape[1]:\n                    mel_spec = mel_spec[:, :self.target_shape[1]]\n                elif mel_spec.shape[1] < self.target_shape[1]:\n                    pad_width = self.target_shape[1] - mel_spec.shape[1]\n                    mel_spec = np.pad(mel_spec, ((0, 0), (0, pad_width)), 'constant', constant_values=mel_spec.min())\n\n                if mel_spec.shape != self.target_shape:\n                    print(f\"Warning: Shape mismatch for {os.path.basename(file_path)}: \"\n                          f\"got {mel_spec.shape}, expected {self.target_shape}. Skipping.\")\n                    continue\n\n                mel_min, mel_max = np.percentile(mel_spec, [1, 99])\n                if mel_max == mel_min:\n                    print(f\"Warning: mel_max equals mel_min for {os.path.basename(file_path)}. Skipping.\")\n                    continue\n\n                mel_norm = np.clip((mel_spec - mel_min) / (mel_max - mel_min + 1e-8), 0, 1)\n                if np.any(np.isnan(mel_norm)):\n                    print(f\"Warning: NaN values detected in mel_norm for {os.path.basename(file_path)}. Skipping.\")\n                    continue\n\n                mel_spectrograms.append(mel_norm)\n                successful_files += 1\n\n            except Exception as e:\n                print(f\"Error processing {file_path}: {e}\")\n                continue\n\n        if len(mel_spectrograms) > 0:\n            mel_spectrograms = np.stack(mel_spectrograms)[:, None, :, :]\n            print(f\"Successfully processed {len(mel_spectrograms)} files\")\n            print(f\"Final data shape: {mel_spectrograms.shape}\")\n            if mel_spectrograms.shape[2:] != self.target_shape:\n                raise ValueError(f\"Stacked spectrograms have incorrect shape: {mel_spectrograms.shape[2:]}, expected {self.target_shape}\")\n            return mel_spectrograms\n        else:\n            raise ValueError(\"No mel spectrogram image files could be processed successfully!\")\n\n    def train_vae(self, mel_spectrograms, epochs=150, batch_size=2, learning_rate=5e-6, beta=0.02, early_stop_patience=50, accum_steps=32):\n        input_shape_vae = mel_spectrograms.shape[2:]\n        print(f\"VAE input shape (H, W): {input_shape_vae}\")\n\n        self.vae = ImprovedAudioVAE(\n            input_shape=input_shape_vae,\n            conv_filters=(16, 32, 64, 256),\n            conv_kernels=(3, 3, 3, 3),\n            conv_strides=(1, 2, 2, 2),\n            latent_dim=256,\n            dropout_rate=0.3\n        ).to(self.device)\n\n        dataset = AudioDataset(mel_spectrograms)\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n\n        optimizer = optim.AdamW(self.vae.parameters(), lr=learning_rate, weight_decay=1e-5)\n        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n        scaler = GradScaler()\n        best_loss = float('inf')\n        patience_counter = 0\n\n        self.vae.train()\n        train_losses = []\n\n        for epoch in range(epochs):\n            total_loss = 0\n            total_recon_loss = 0\n            total_kl_loss = 0\n            progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{epochs}')\n\n            optimizer.zero_grad(set_to_none=True)\n            for batch_idx, data in enumerate(progress_bar):\n                data = data.to(self.device, non_blocking=True)\n\n                with autocast():\n                    recon_batch, mu, logvar = self.vae(data)\n                    # Debug: Check for NaN in recon_batch\n                    if torch.any(torch.isnan(recon_batch)):\n                        print(f\"NaN detected in recon_batch at batch {batch_idx+1}\")\n                        raise ValueError(\"NaN detected in recon_batch\")\n                    recon_loss, kl_loss = self.improved_vae_loss(recon_batch, data, mu, logvar, beta=beta)\n                    loss = (recon_loss + kl_loss) / accum_steps\n\n                scaler.scale(loss).backward()\n                if (batch_idx + 1) % accum_steps == 0 or (batch_idx + 1) == len(dataloader):\n                    scaler.unscale_(optimizer)\n                    # Debug: Compute and print gradient norm\n                    grad_norm = torch.nn.utils.clip_grad_norm_(self.vae.parameters(), max_norm=float('inf'))\n                    print(f\"Batch {batch_idx+1}: Gradient norm before clipping = {grad_norm:.4f}\")\n                    torch.nn.utils.clip_grad_norm_(self.vae.parameters(), max_norm=0.1)\n                    scaler.step(optimizer)\n                    scaler.update()\n                    optimizer.zero_grad(set_to_none=True)\n\n                total_loss += loss.item() * accum_steps\n                total_recon_loss += recon_loss.item()\n                total_kl_loss += kl_loss.item()\n\n                progress_bar.set_postfix({\n                    'loss': f'{loss.item() * accum_steps:.4f}',\n                    'recon': f'{recon_loss.item():.4f}',\n                    'kl': f'{kl_loss.item():.4f}',\n                    'mu_mean': f'{mu.mean().item():.4f}',\n                    'logvar_mean': f'{logvar.mean().item():.4f}'\n                })\n\n                if torch.isnan(loss) or torch.isnan(recon_loss) or torch.isnan(kl_loss):\n                    print(f\"NaN detected in batch {batch_idx+1}: Loss={loss.item()}, Recon={recon_loss.item()}, KL={kl_loss.item()}\")\n                    print(f\"Mu stats: mean={mu.mean().item():.4f}, max={mu.max().item():.4f}, min={mu.min().item():.4f}\")\n                    print(f\"Logvar stats: mean={logvar.mean().item():.4f}, max={logvar.max().item():.4f}, min={logvar.min().item():.4f}\")\n                    raise ValueError(\"Training stopped due to NaN loss.\")\n\n                del data, recon_batch, mu, logvar, loss, recon_loss, kl_loss\n                gc.collect()\n                torch.cuda.empty_cache()\n\n            avg_loss = total_loss / len(dataloader)\n            avg_recon_loss = total_recon_loss / len(dataloader)\n            avg_kl_loss = total_kl_loss / len(dataloader)\n\n            train_losses.append(avg_loss)\n            scheduler.step()\n\n            if avg_loss < best_loss:\n                best_loss = avg_loss\n                patience_counter = 0\n                os.makedirs(\"models\", exist_ok=True)\n                torch.save(self.vae.state_dict(), 'models/best_vae_model.pth')\n                print(\"Best VAE model saved.\")\n            else:\n                patience_counter += 1\n\n            if epoch % 10 == 0 or patience_counter == 0:\n                print(f'Epoch {epoch+1:3d} | Loss: {avg_loss:.4f} | '\n                      f'Recon: {avg_recon_loss:.4f} | KL: {avg_kl_loss:.4f} | '\n                      f'LR: {optimizer.param_groups[0][\"lr\"]:.6f} | '\n                      f'Patience: {patience_counter}/{early_stop_patience}')\n\n            if patience_counter >= early_stop_patience:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                break\n\n        if os.path.exists('models/best_vae_model.pth'):\n            self.vae.load_state_dict(torch.load('models/best_vae_model.pth'))\n            print(\"Best VAE model loaded.\")\n        else:\n            print(\"Warning: Best model checkpoint not found.\")\n\n        print(\"VAE training completed!\")\n        self.plot_training_curve(train_losses)\n\n    def improved_vae_loss(self, recon_x, x, mu, logvar, beta):\n        recon_loss = nn.functional.mse_loss(recon_x, x, reduction='sum') / x.size(0)\n        # Clamp both mu and logvar to prevent numerical issues\n        mu = torch.clamp(mu, min=-10, max=10)\n        logvar = torch.clamp(logvar, min=-20, max=2)\n        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)\n        return recon_loss, beta * kl_loss\n\n    def plot_training_curve(self, train_losses):\n        plt.figure(figsize=(10, 6))\n        plt.plot(train_losses, label='Training Loss')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.title('VAE Training Progress')\n        plt.legend()\n        plt.grid(True)\n        plt.show()\n\n    def generate_deepfake(self, original_mel_spec, noise_levels=[0.0, 0.1, 0.2]):\n        if self.vae is None:\n            print(\"Error: VAE model not trained. Please train the model first.\")\n            return None\n\n        self.vae.eval()\n        deepfake_results = []\n\n        with torch.no_grad():\n            if original_mel_spec.shape != self.target_shape:\n                print(f\"Warning: Input mel shape {original_mel_spec.shape} does not match VAE target shape {self.target_shape}. Resizing.\")\n                original_mel_spec = np.resize(original_mel_spec, self.target_shape)\n\n            mel_min, mel_max = np.percentile(original_mel_spec, [1, 99])\n            if mel_max == mel_min:\n                print(\"Error: Invalid mel spectrogram range for generation (mel_max equals mel_min).\")\n                return original_mel_spec\n\n            mel_norm = np.clip((original_mel_spec - mel_min) / (mel_max - mel_min + 1e-8), 0, 1)\n            input_tensor = torch.FloatTensor(mel_norm[None, None, :, :]).to(self.device, non_blocking=True)\n\n            mu, logvar = self.vae.encode(input_tensor)\n            for noise_level in noise_levels:\n                noise = torch.randn_like(mu, device=self.device) * noise_level\n                z_modified = mu + noise\n                fake_mel_norm = self.vae.decode(z_modified).cpu().numpy().squeeze()\n                fake_mel_norm = np.clip(fake_mel_norm, 0, 1)\n                fake_mel = fake_mel_norm * (mel_max - mel_min) + mel_min\n                deepfake_results.append((fake_mel, noise_level))\n\n            del input_tensor, mu, logvar, noise, z_modified, fake_mel_norm\n            gc.collect()\n            torch.cuda.empty_cache()\n\n        return deepfake_results\n\n    def evaluate_model(self, test_spectrograms, beta=0.02):\n        if self.vae is None:\n            print(\"Error: VAE model not trained for evaluation.\")\n            return None\n\n        self.vae.eval()\n        total_loss = 0\n        total_recon_loss = 0\n        total_kl_loss = 0\n        total_samples = 0\n        with torch.no_grad():\n            dataloader = DataLoader(AudioDataset(test_spectrograms), batch_size=2, shuffle=False, num_workers=0)\n            for batch_idx, batch in enumerate(dataloader):\n                batch_tensor = batch.to(self.device, non_blocking=True)\n                with autocast():\n                    recon_batch, mu, logvar = self.vae(batch_tensor)\n                    recon_loss, kl_loss = self.improved_vae_loss(recon_batch, batch_tensor, mu, logvar, beta=beta)\n                total_loss += (recon_loss + kl_loss).item() * len(batch)\n                total_recon_loss += recon_loss.item() * len(batch)\n                total_kl_loss += kl_loss.item() * len(batch)\n                total_samples += len(batch)\n                del batch_tensor, recon_batch, mu, logvar, recon_loss, kl_loss\n                gc.collect()\n                torch.cuda.empty_cache()\n\n        if total_samples > 0:\n            avg_loss = total_loss / total_samples\n            avg_recon_loss = total_recon_loss / total_samples\n            avg_kl_loss = total_kl_loss / total_samples\n            print(f\"Evaluation Loss: {avg_loss:.4f} (Recon: {avg_recon_loss:.4f}, KL: {avg_kl_loss:.4f})\")\n            return avg_loss, avg_recon_loss, avg_kl_loss\n        else:\n            print(\"No test samples available for evaluation.\")\n            return None\n\n    def compare_audio(self, original_mel, fake_mel):\n        min_height = min(original_mel.shape[0], fake_mel.shape[0])\n        min_width = min(original_mel.shape[1], fake_mel.shape[1])\n        original_mel_clipped = original_mel[:min_height, :min_width]\n        fake_mel_clipped = fake_mel[:min_height, :min_width]\n\n        orig_flat = original_mel_clipped.flatten()\n        fake_flat = fake_mel_clipped.flatten()\n\n        mse = mean_squared_error(orig_flat, fake_flat)\n        if np.all(orig_flat == orig_flat[0]) or np.all(fake_flat == fake_flat[0]):\n            cosine_sim = np.nan\n            correlation = np.nan\n        else:\n            cosine_sim = 1 - cosine(orig_flat, fake_flat)\n            correlation = np.corrcoef(orig_flat, fake_flat)[0, 1]\n\n        print(\"=== Audio Comparison Results ===\")\n        print(f\"Mean Squared Error: {mse:.4f}\")\n        print(f\"Cosine Similarity: {cosine_sim:.4f}\")\n        print(f\"Correlation: {correlation:.4f}\")\n        return {'mse': mse, 'cosine_similarity': cosine_sim, 'correlation': correlation}\n\n    def visualize_comparison(self, original_mel, fake_mel):\n        min_height = min(original_mel.shape[0], fake_mel.shape[0])\n        min_width = min(original_mel.shape[1], fake_mel.shape[1])\n        original_mel_clipped = original_mel[:min_height, :min_width]\n        fake_mel_clipped = fake_mel[:min_height, :min_width]\n\n        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n        im1 = axes[0].imshow(original_mel_clipped, aspect='auto', origin='lower', cmap='viridis')\n        axes[0].set_title('Original Mel Spectrogram')\n        axes[0].set_xlabel('Time')\n        axes[0].set_ylabel('Mel Frequency')\n        plt.colorbar(im1, ax=axes[0])\n        im2 = axes[1].imshow(fake_mel_clipped, aspect='auto', origin='lower', cmap='viridis')\n        axes[1].set_title('Generated Deepfake Mel Spectrogram')\n        axes[1].set_xlabel('Time')\n        axes[1].set_ylabel('Mel Frequency')\n        plt.colorbar(im2, ax=axes[1])\n        diff = np.abs(original_mel_clipped - fake_mel_clipped)\n        im3 = axes[2].imshow(diff, aspect='auto', origin='lower', cmap='hot')\n        axes[2].set_title('Absolute Difference')\n        axes[2].set_xlabel('Time')\n        axes[2].set_ylabel('Mel Frequency')\n        plt.colorbar(im3, ax=axes[2])\n        plt.tight_layout()\n        plt.show()\n\n    def save_audio(self, mel_spec, filename, sr=22050):\n        audio = self.processor.mel_spectrogram_to_audio(mel_spec)\n        sf.write(filename, audio, sr)\n        print(f\"Audio saved as: {filename}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Main Function","metadata":{}},{"cell_type":"code","source":"def get_image_files(directory):\n    image_files = []\n    valid_extensions = ('.png', '.jpg', '.jpeg')\n    if not os.path.exists(directory):\n        print(f\"Directory {directory} does not exist\")\n        return image_files\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith(valid_extensions):\n                file_path = os.path.join(root, file)\n                try:\n                    img = Image.open(file_path).convert('L')\n                    img_array = np.array(img, dtype=np.float32)\n                    image_files.append((file_path, img_array))\n                except Exception as e:\n                    print(f\"Error loading image {file_path}: {e}\")\n    return image_files\n\ndef main():\n    torch.cuda.empty_cache()\n    generator = AudioDeepfakeGenerator(height=128, width=128)\n    base_path = \"/kaggle/input/vae128-ctrsdd\"\n    train_real_path = os.path.join(base_path, \"spec_128\")\n    \n    train_files = get_image_files(train_real_path)\n    if not train_files:\n        print(\"No image files found in the training directory.\")\n        return\n\n    train_files, test_files = train_test_split(train_files, test_size=0.2, random_state=42)\n    val_files = test_files\n\n    print(f\"Found {len(train_files)} training files\")\n    print(f\"Found {len(val_files)} validation files\")\n    print(f\"Found {len(test_files)} testing files\")\n\n    print(\"\\nStep 1: Processing training mel spectrograms...\")\n    try:\n        train_spectrograms = generator.preprocess_mel_spectrograms(train_files)\n    except ValueError as e:\n        print(f\"Error: {e}\")\n        return\n\n    print(\"\\nStep 2: Processing validation mel spectrograms...\")\n    try:\n        val_spectrograms = generator.preprocess_mel_spectrograms(val_files)\n    except ValueError as e:\n        print(f\"Error: {e}\")\n        return\n\n    print(\"\\nStep 3: Processing testing mel spectrograms...\")\n    try:\n        test_spectrograms = generator.preprocess_mel_spectrograms(test_files)\n    except ValueError as e:\n        print(f\"Error: {e}\")\n        return\n\n    print(f\"Training samples: {len(train_spectrograms)}\")\n    print(f\"Validation samples: {len(val_spectrograms)}\")\n    print(f\"Testing samples: {len(test_spectrograms)}\")\n\n    print(\"\\nStep 4: Training convolutional VAE...\")\n    generator.train_vae(\n        mel_spectrograms=train_spectrograms,\n        epochs=250,\n        batch_size=32,\n        learning_rate=5e-6,\n        beta=0.02,\n        early_stop_patience=50,\n        accum_steps=32\n    )\n\n    print(\"\\nStep 5: Evaluating model on validation set...\")\n    val_metrics = generator.evaluate_model(val_spectrograms, beta=0.02)\n    if val_metrics:\n        print(f\"Validation Metrics: Total Loss: {val_metrics[0]:.4f}, Recon Loss: {val_metrics[1]:.4f}, KL Loss: {val_metrics[2]:.4f}\")\n\n    print(\"\\nStep 6: Evaluating model on test set...\")\n    test_metrics = generator.evaluate_model(test_spectrograms, beta=0.02)\n    if test_metrics:\n        print(f\"Test Metrics: Total Loss: {test_metrics[0]:.4f}, Recon Loss: {test_metrics[1]:.4f}, KL Loss: {test_metrics[2]:.4f}\")\n\n    for j in range(11):\n        print(\"\\nStep 7: Preparing reference for deepfake generation...\")\n        reference_file, reference_mel = test_files[j]\n        if reference_mel.shape != generator.target_shape:\n            if reference_mel.shape[0] > generator.target_shape[0]:\n                reference_mel = reference_mel[:generator.target_shape[0], :]\n            elif reference_mel.shape[0] < generator.target_shape[0]:\n                pad_height = generator.target_shape[0] - reference_mel.shape[0]\n                reference_mel = np.pad(reference_mel, ((0, pad_height), (0, 0)), 'constant', constant_values=reference_mel.min())\n    \n            if reference_mel.shape[1] > generator.target_shape[1]:\n                reference_mel = reference_mel[:, :self.target_shape[1]]\n            elif reference_mel.shape[1] < generator.target_shape[1]:\n                pad_width = generator.target_shape[1] - reference_mel.shape[1]\n                reference_mel = np.pad(reference_mel, ((0, 0), (0, pad_width)), 'constant', constant_values=reference_mel.min())\n    \n        print(f\"Reference file: {os.path.basename(reference_file)}, shape: {reference_mel.shape}\")\n        generator.processor.plot_mel_spectrogram(reference_mel, \"Original Reference Mel Spectrogram\")\n    \n        print(\"\\nStep 8: Generating high-quality deepfakes...\")\n        noise_levels = [0.0, 0.1, 0.2]\n        deepfake_results = generator.generate_deepfake(reference_mel, noise_levels=noise_levels)\n        if deepfake_results is None:\n            continue\n    \n        for i, (fake_mel, noise_level) in enumerate(deepfake_results):\n            print(f\"Generated deepfake variation {i+1}/{len(deepfake_results)} (noise={noise_level}) for sample {j+1}...\")\n            generator.save_audio(fake_mel, f\"high_quality_deepfake_sample{j+1}_noise_{noise_level}.wav\")\n            results = generator.compare_audio(reference_mel, fake_mel)\n            print(f\"  MSE: {results['mse']:.4f}, Cosine Sim: {results['cosine_similarity']:.4f}, Correlation: {results['correlation']:.4f}\")\n    \n        best_idx = 0\n        best_fake_mel, best_noise = deepfake_results[best_idx]\n    \n        print(f\"\\nStep 9: Detailed analysis of best deepfake (noise={best_noise}) for sample {j+1}...\")\n        detailed_results = generator.compare_audio(reference_mel, best_fake_mel)\n    \n        print(\"\\nStep 10: Visualizing results...\")\n        generator.visualize_comparison(reference_mel, best_fake_mel)\n        generator.save_audio(reference_mel, f\"original_reference_sample{j+1}.wav\")\n    \n        print(f\"\\nImproved deepfake generation completed for sample {j+1}!\")\n        print(f\"Generated {len(deepfake_results)} high-quality variations\")\n        print(\"Files saved:\")\n        print(f\"- original_reference_sample{j+1}.wav\")\n        for i, (_, noise) in enumerate(deepfake_results):\n            print(f\"- high_quality_deepfake_sample{j+1}_noise_{noise}.wav\")\n\n    print(f\"\\nPerformance Summary for sample {j+1}:\")\n    print(f\"Best deepfake metrics:\")\n    print(f\"  MSE: {detailed_results['mse']:.6f}\")\n    print(f\"  Cosine Similarity: {detailed_results['cosine_similarity']:.6f}\")\n    print(f\"  Correlation: {detailed_results['correlation']:.6f}\")\n\nif __name__ == \"__main__\":\n    torch.cuda.empty_cache()\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}