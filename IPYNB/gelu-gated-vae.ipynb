{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install PyTorch and related libraries ","metadata":{}},{"cell_type":"code","source":"!pip install torch torchvision torchaudio","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  Setup and Imports\n\n- Sets a CUDA memory config (`expandable_segments:True`) to prevent out-of-memory errors during dynamic allocation in PyTorch.\n- Imports essential libraries for:\n  -  **Audio processing**: `librosa`, `torchaudio`, `soundfile`\n  -  **Visualization**: `matplotlib`, `librosa.display`\n  -  **Deep learning**: `torch`, `torch.nn`, `torch.optim`, `amp` for mixed-precision training\n  -  **Model evaluation**: `mean_squared_error`, `cosine` distance\n  -  **Data management**: `Dataset`, `DataLoader`, `train_test_split`\n  -  **Others**: `tqdm` for progress bars, `PIL` for image operations, `warnings` to suppress logs\n","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\nimport math\nimport librosa\nimport librosa.display\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.cuda.amp as amp\nimport soundfile as sf\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.spatial.distance import cosine\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.auto import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" # GeluGatedLayer","metadata":{}},{"cell_type":"code","source":"class GeluGatedLayer(nn.Module):\n    def __init__(self, input_dim, output_dim, bias=True):\n        super(GeluGatedLayer, self).__init__()\n        self.input_linear = nn.Linear(input_dim, output_dim, bias=bias)\n        self.activation = nn.GELU()\n\n    def forward(self, src):\n        output = self.activation(self.input_linear(src))\n        return output","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# AudioProcessor: Audio Feature Utility Class\n\nThis helper class handles key audio preprocessing and visualization tasks.\n\n**Core Methods:**\n- `mel_spectrogram_to_audio`:  \n  ðŸ” Converts a Mel spectrogram (in dB) back into waveform audio using:\n  - `librosa.db_to_power()` â†’ converts dB to power\n  - `mel_to_stft()` â†’ approximates original STFT\n  - `griffinlim()` â†’ reconstructs waveform via iterative phase estimation\n\n- `plot_mel_spectrogram`:  \n  ðŸ“Š Plots the given Mel spectrogram using `librosa.display.specshow()`  \n  - Adds axes, color bar, and title for easy interpretation","metadata":{}},{"cell_type":"code","source":"# AudioProcessor class\nclass AudioProcessor:\n    def __init__(self, sr=22050, n_mels=128, n_fft=2048, hop_length=512):\n        self.sr = sr\n        self.n_mels = n_mels\n        self.n_fft = n_fft\n        self.hop_length = hop_length\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    def mel_spectrogram_to_audio(self, mel_spec_db):\n        mel_spec = librosa.db_to_power(mel_spec_db)\n        stft = librosa.feature.inverse.mel_to_stft(mel_spec, sr=self.sr, n_fft=self.n_fft)\n        audio = librosa.griffinlim(stft, hop_length=self.hop_length)\n        return audio\n\n    def plot_mel_spectrogram(self, mel_spec_db, title=\"Mel Spectrogram\"):\n        plt.figure(figsize=(12, 6))\n        librosa.display.specshow(\n            mel_spec_db, sr=self.sr, hop_length=self.hop_length, x_axis='time', y_axis='mel'\n        )\n        plt.colorbar(format='%+2.0f dB')\n        plt.title(title)\n        plt.tight_layout()\n        plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# AudioDataset: PyTorch Dataset Wrapper for Mel Spectrograms","metadata":{}},{"cell_type":"code","source":"# AudioDataset class\nclass AudioDataset(Dataset):\n    def __init__(self, mel_spectrograms):\n        self.mel_spectrograms = mel_spectrograms\n\n    def __len__(self):\n        return len(self.mel_spectrograms)\n\n    def __getitem__(self, idx):\n        return torch.FloatTensor(self.mel_spectrograms[idx])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ImprovedAudioVAE: Audio VAE with GELU-Gated Layers\n\nThis version of `ImprovedAudioVAE` enhances the encoder with **GELU-gated layers** (via `GeluGatedLayer`)\n---\n\n###  Initialization (`__init__`)\n- Initializes VAE parameters and builds:\n  - `Encoder` with GELU gating\n  - `Bottleneck`\n  - `Decoder` using transposed convolutions","metadata":{}},{"cell_type":"code","source":"class ImprovedAudioVAE(nn.Module):\n    def __init__(self, input_shape, conv_filters=(16, 32, 64, 64), conv_kernels=(3, 3, 3, 3),\n                 conv_strides=(1, 2, 2, 2), latent_dim=128, dropout_rate=0.2):\n        super(ImprovedAudioVAE, self).__init__()\n\n        self.input_shape = input_shape\n        self.conv_filters = conv_filters\n        self.conv_kernels = conv_kernels\n        self.conv_strides = conv_strides\n        self.latent_dim = latent_dim\n        self.dropout_rate = dropout_rate\n        self.num_conv_layers = len(conv_filters)\n        self.shape_before_bottleneck = None\n\n        self.encoder = self._build_encoder()\n        self.decoder = self._build_decoder()\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear, GeluGatedLayer)):\n            if hasattr(module, 'weight') and module.weight is not None:\n                torch.nn.init.xavier_uniform_(module.weight)\n            if hasattr(module, 'bias') and module.bias is not None:\n                torch.nn.init.constant_(module.bias, 0)\n\n    def _build_encoder(self):\n        layers = []\n        self.gelu_positions = []\n        self.gelu_layers = nn.ModuleList()\n        in_channels = 1\n\n        for i, (filters, kernel, stride) in enumerate(zip(self.conv_filters, self.conv_kernels, self.conv_strides)):\n            layers.extend([\n                nn.Conv2d(\n                    in_channels=in_channels,\n                    out_channels=filters,\n                    kernel_size=kernel,\n                    stride=stride,\n                    padding=kernel // 2,\n                ),\n                nn.ReLU(),\n                nn.BatchNorm2d(filters),\n                nn.Dropout(self.dropout_rate)\n            ])\n            self.gelu_positions.append(len(layers) - 1)\n            self.gelu_layers.append(GeluGatedLayer(filters, filters))\n            in_channels = filters\n\n        with torch.no_grad():\n            dummy_input = torch.zeros(1, 1, *self.input_shape)\n            x = dummy_input\n            gelu_idx = 0\n            for i, layer in enumerate(layers):\n                x = layer(x)\n                if i in self.gelu_positions:\n                    b, c, h, w = x.shape\n                    x_flat = x.permute(0, 2, 3, 1).reshape(-1, c)\n                    x_flat = self.gelu_layers[gelu_idx](x_flat)\n                    x = x_flat.reshape(b, h, w, c).permute(0, 3, 1, 2)\n                    gelu_idx += 1\n            self.shape_before_bottleneck = x.shape[1:]\n\n        flat_dim = np.prod(self.shape_before_bottleneck)\n        layers.append(nn.Flatten())\n        self.mu = nn.Linear(flat_dim, self.latent_dim)\n        self.logvar = nn.Linear(flat_dim, self.latent_dim)\n\n        self.encoder_core = nn.Sequential(*layers)\n        return self.encoder_core\n\n    def _build_decoder(self):\n        layers = []\n        num_neurons = np.prod(self.shape_before_bottleneck)\n        layers.extend([\n            nn.Linear(self.latent_dim, num_neurons),\n            nn.LayerNorm(num_neurons),\n            nn.Unflatten(1, self.shape_before_bottleneck)\n        ])\n\n        in_channels = self.conv_filters[-1]\n        for i in reversed(range(1, self.num_conv_layers)):\n            layers.extend([\n                nn.ConvTranspose2d(\n                    in_channels=in_channels,\n                    out_channels=self.conv_filters[i-1],\n                    kernel_size=self.conv_kernels[i],\n                    stride=self.conv_strides[i],\n                    padding=self.conv_kernels[i] // 2,\n                    output_padding=0\n                ),\n                nn.ReLU(),\n                nn.BatchNorm2d(self.conv_filters[i-1]),\n                nn.Dropout(self.dropout_rate)\n            ])\n            in_channels = self.conv_filters[i-1]\n\n        layers.extend([\n            nn.ConvTranspose2d(\n                in_channels=in_channels,\n                out_channels=1,\n                kernel_size=self.conv_kernels[0],\n                stride=self.conv_strides[0],\n                padding=self.conv_kernels[0] // 2,\n                output_padding=0\n            ),\n            nn.Sigmoid()\n        ])\n\n        return nn.Sequential(*layers)\n\n    def encode(self, x):\n        if x.dim() == 3:\n            x = x.unsqueeze(1)\n        elif x.dim() == 2:\n            x = x.unsqueeze(0).unsqueeze(0)\n        h = checkpoint.checkpoint_sequential(self.encoder, segments=2, input=x)\n        mu = self.mu(h)\n        logvar = self.logvar(h)\n        return mu, logvar\n\n    def decode(self, z):\n        x = self.decoder(z)\n        if x.dim() == 3:\n            x = x.unsqueeze(1)\n        recon_x = torch.nn.functional.interpolate(x, size=self.input_shape, mode='bilinear', align_corners=False)\n        return recon_x\n\n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def forward(self, x):\n        if x.dim() == 3:\n            x = x.unsqueeze(1)\n        elif x.dim() == 2:\n            x = x.unsqueeze(0).unsqueeze(0)\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        recon_x = self.decode(z)\n        return recon_x, mu, logvar","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  AudioDeepfakeGenerator: End-to-End Deepfake Audio Generator Using VAE\n\n\nThis class orchestrates the full workflow for generating deepfake audio using a **Variational Autoencoder (VAE)** trained on Mel spectrograms.\n\n---\n\n### Initialization (`__init__`)\n- Sets up:\n  - `AudioProcessor` (Mel spectrogram utils)\n  - VAE model placeholder\n  - `GradScaler` for mixed-precision training\n- Target input shape defaults to 128x128\n\n---\n\n### Preprocessing (`preprocess_mel_spectrograms`)\n- Pads or crops Mel spectrograms to target shape\n- Normalizes values between 0â€“1 using robust min-max scaling\n- Converts valid spectrograms into a 4D NumPy array for training\n\n---\n\n### VAE Training (`train_vae`)\n- Initializes and trains an `ImprovedAudioVAE` model:\n  - Customizable epochs, learning rate, batch size, KL-Î² weight\n  - Uses gradient accumulation + mixed precision\n  - Saves best + periodic model checkpoints\n  - Applies early stopping based on validation loss\n\n---\n\n### VAE Loss (`improved_vae_loss`)\n- Combines:\n  - MSE reconstruction loss\n  - KL divergence loss (scaled by `Î²`)\n\n---\n\n### Visualization (`plot_training_curve`)\n- Plots training loss curve over epochs\n\n---\n\n### Deepfake Generation (`generate_deepfake`)\n- Uses a trained VAE to reconstruct + modify original Mel spectrograms\n- Adds controllable Gaussian noise in latent space\n- Returns reconstructed Mel spectrograms for each noise level\n\n---\n\n### Evaluation (`evaluate_model`)\n- Computes average loss on a test set using same VAE loss\n- Useful for monitoring model generalization\n\n---\n\n### Comparison (`compare_audio`)\n- Compares original and fake Mel spectrograms:\n  - MSE (Mean Squared Error)\n  - Cosine similarity\n  - Pearson correlation\n\n---\n\n### Visualization (`visualize_comparison`)\n- Plots:\n  - Original Mel\n  - Generated Mel\n  - Absolute difference between them\n\n---\n\n### Audio Output (`save_audio`)\n- Converts a Mel spectrogram back to waveform\n- Saves output using `soundfile.write`\n\n---\n","metadata":{}},{"cell_type":"code","source":"\n# AudioDeepfakeGenerator class\nclass AudioDeepfakeGenerator:\n    def __init__(self, height=128, width=128):\n        self.processor = AudioProcessor(n_mels=height, sr=22050, n_fft=2048, hop_length=512)\n        self.vae = None\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        print(f\"Using device: {self.device}\")\n        self.target_shape = (height, width)\n        self.scaler = amp.GradScaler(enabled=self.device.type == 'cuda')\n\n    def preprocess_mel_spectrograms(self, file_data):\n        mel_spectrograms = []\n        successful_files = 0\n        print(f\"Processing {len(file_data)} mel spectrogram image files...\")\n\n        for file_path, mel_spec in file_data:\n            try:\n                if mel_spec.shape[0] > self.target_shape[0]:\n                    mel_spec = mel_spec[:self.target_shape[0], :]\n                elif mel_spec.shape[0] < self.target_shape[0]:\n                    pad_height = self.target_shape[0] - mel_spec.shape[0]\n                    mel_spec = np.pad(mel_spec, ((0, pad_height), (0, 0)), 'constant', constant_values=mel_spec.min())\n\n                if mel_spec.shape[1] > self.target_shape[1]:\n                    mel_spec = mel_spec[:, :self.target_shape[1]]\n                elif mel_spec.shape[1] < self.target_shape[1]:\n                    pad_width = self.target_shape[1] - mel_spec.shape[1]\n                    mel_spec = np.pad(mel_spec, ((0, 0), (0, pad_width)), 'constant', constant_values=mel_spec.min())\n\n                if mel_spec.shape != self.target_shape:\n                    print(f\"Warning: Shape mismatch for {os.path.basename(file_path)}: \"\n                          f\"got {mel_spec.shape}, expected {self.target_shape}. Skipping.\")\n                    continue\n\n                mel_min, mel_max = np.percentile(mel_spec, [1, 99])\n                if mel_max <= mel_min:\n                    print(f\"Warning: Invalid mel spectrogram range for {os.path.basename(file_path)}. Skipping.\")\n                    continue\n\n                mel_norm = np.clip((mel_spec - mel_min) / (mel_max - mel_min + 1e-8), 0, 1)\n                mel_spectrograms.append(mel_norm)\n                successful_files += 1\n\n            except Exception as e:\n                print(f\"Error processing {file_path}: {e}\")\n                continue\n\n        if len(mel_spectrograms) > 0:\n            mel_spectrograms = np.stack(mel_spectrograms)[:, None, :, :]\n            print(f\"Successfully processed {len(mel_spectrograms)} files\")\n            print(f\"Final data shape: {mel_spectrograms.shape}\")\n            if mel_spectrograms.shape[2:] != self.target_shape:\n                raise ValueError(f\"Stacked spectrograms have incorrect shape: {mel_spectrograms.shape[2:]}, expected {self.target_shape}\")\n            return mel_spectrograms\n        else:\n            raise ValueError(\"No mel spectrogram image files could be processed successfully!\")\n\n    def train_vae(self, mel_spectrograms, epochs=5, batch_size=1, learning_rate=1e-4, beta=0.1, early_stop_patience=60, accum_steps=2):\n        torch.cuda.empty_cache()\n        input_shape_vae = mel_spectrograms.shape[2:]\n        print(f\"VAE input shape (H, W): {input_shape_vae}\")\n\n        # Create models directory early\n        os.makedirs(\"models\", exist_ok=True)\n\n        self.vae = ImprovedAudioVAE(\n            input_shape=input_shape_vae,\n            conv_filters=(16, 32, 64, 256),\n            conv_kernels=(3, 3, 3, 3),\n            conv_strides=(1, 2, 2, 2),\n            latent_dim=128,\n            dropout_rate=0.2\n        ).to(self.device)\n\n        dataset = AudioDataset(mel_spectrograms)\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n\n        optimizer = optim.AdamW(self.vae.parameters(), lr=learning_rate, weight_decay=1e-5)\n        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n        best_loss = float('inf')\n        patience_counter = 0\n\n        self.vae.train()\n        train_losses = []\n\n        for epoch in range(epochs):\n            total_loss = 0\n            total_recon_loss = 0\n            total_kl_loss = 0\n            progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{epochs}')\n            optimizer.zero_grad(set_to_none=True)\n\n            for batch_idx, data in enumerate(progress_bar):\n                data = data.to(self.device, non_blocking=True)\n\n                with amp.autocast(enabled=self.device.type == 'cuda'):\n                    recon_batch, mu, logvar = self.vae(data)\n                    recon_loss, kl_loss = self.improved_vae_loss(recon_batch, data, mu, logvar, beta=beta)\n                    loss = (recon_loss + kl_loss) / accum_steps\n\n                self.scaler.scale(loss).backward()\n\n                if (batch_idx + 1) % accum_steps == 0 or (batch_idx + 1) == len(dataloader):\n                    self.scaler.unscale_(optimizer)\n                    torch.nn.utils.clip_grad_norm_(self.vae.parameters(), max_norm=1.0)\n                    self.scaler.step(optimizer)\n                    self.scaler.update()\n                    optimizer.zero_grad(set_to_none=True)\n\n                total_loss += loss.item() * accum_steps\n                total_recon_loss += recon_loss.item()\n                total_kl_loss += kl_loss.item()\n\n                progress_bar.set_postfix({\n                    'loss': f'{loss.item() * accum_steps:.4f}',\n                    'recon': f'{recon_loss.item():.4f}',\n                    'kl': f'{kl_loss.item():.4f}'\n                })\n\n                del data, recon_batch, mu, logvar, loss\n                torch.cuda.empty_cache()\n\n            avg_loss = total_loss / len(dataloader)\n            avg_recon_loss = total_recon_loss / len(dataloader)\n            avg_kl_loss = total_kl_loss / len(dataloader)\n\n            train_losses.append(avg_loss)\n            scheduler.step()\n\n            # Log loss details to diagnose issues\n            print(f'Epoch {epoch+1:3d} | Avg Loss: {avg_loss:.4f} | '\n                  f'Recon Loss: {avg_recon_loss:.4f} | KL Loss: {avg_kl_loss:.4f} | '\n                  f'Best Loss: {best_loss:.4f}')\n\n            # Save checkpoint if loss improves\n            if avg_loss < best_loss:\n                best_loss = avg_loss\n                patience_counter = 0\n                try:\n                    torch.save(self.vae.state_dict(), 'models/best_vae_model.pth')\n                    print(f\"Saved best model checkpoint at epoch {epoch+1}\")\n                except Exception as e:\n                    print(f\"Error saving checkpoint: {e}\")\n            else:\n                patience_counter += 1\n\n            # Save periodic checkpoint every 2 epochs\n            if (epoch + 1) % 2 == 0:\n                try:\n                    torch.save(self.vae.state_dict(), f'models/vae_model_epoch_{epoch+1}.pth')\n                    print(f\"Saved periodic checkpoint at epoch {epoch+1}\")\n                except Exception as e:\n                    print(f\"Error saving periodic checkpoint: {e}\")\n\n            if epoch % 2 == 0 or patience_counter == 0:\n                print(f'Epoch {epoch+1:3d} | Loss: {avg_loss:.4f} | '\n                      f'Recon: {avg_recon_loss:.4f} | KL: {avg_kl_loss:.4f} | '\n                      f'LR: {optimizer.param_groups[0][\"lr\"]:.6f} | '\n                      f'Patience: {patience_counter}/{early_stop_patience}')\n\n            if patience_counter >= early_stop_patience:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                break\n\n            torch.cuda.empty_cache()\n\n        # Save final model if no checkpoint was saved\n        if not os.path.exists('models/best_vae_model.pth'):\n            try:\n                torch.save(self.vae.state_dict(), 'models/best_vae_model.pth')\n                print(\"Saved final model checkpoint\")\n            except Exception as e:\n                print(f\"Error saving final checkpoint: {e}\")\n\n        # Load best model if it exists, otherwise continue with current state\n        if os.path.exists('models/best_vae_model.pth'):\n            try:\n                self.vae.load_state_dict(torch.load('models/best_vae_model.pth'))\n                print(\"Best VAE model loaded.\")\n            except Exception as e:\n                print(f\"Error loading checkpoint: {e}. Continuing with current model state.\")\n        else:\n            print(\"No checkpoint found. Using current model state.\")\n\n        print(\"VAE training completed!\")\n        self.plot_training_curve(train_losses)\n        torch.cuda.empty_cache()\n\n    def improved_vae_loss(self, recon_x, x, mu, logvar, beta):\n        recon_loss = nn.functional.mse_loss(recon_x, x, reduction='sum') / x.size(0)\n        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)\n        return recon_loss, beta * kl_loss\n\n    def plot_training_curve(self, train_losses):\n        plt.figure(figsize=(10, 6))\n        plt.plot(train_losses, label='Training Loss')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.title('VAE Training Progress')\n        plt.legend()\n        plt.grid(True)\n        plt.close()\n\n    def generate_deepfake(self, original_mel_spec, noise_levels=[0.0, 0.1]):\n        if self.vae is None:\n            print(\"Error: VAE model not trained. Please train the model first.\")\n            return None\n\n        self.vae.eval()\n        deepfake_results = []\n\n        with torch.no_grad():\n            if original_mel_spec.shape != self.target_shape:\n                print(f\"Warning: Input mel shape {original_mel_spec.shape} does not match VAE target shape {self.target_shape}. Resizing.\")\n                original_mel_spec = np.resize(original_mel_spec, self.target_shape)\n\n            mel_min, mel_max = np.percentile(original_mel_spec, [1, 99])\n            if mel_max <= mel_min:\n                print(\"Error: Invalid mel spectrogram range for generation.\")\n                return original_mel_spec\n\n            mel_norm = np.clip((original_mel_spec - mel_min) / (mel_max - mel_min + 1e-8), 0, 1)\n            input_tensor = torch.FloatTensor(mel_norm[None, None, :, :]).to(self.device, non_blocking=True)\n\n            mu, logvar = self.vae.encode(input_tensor)\n            for noise_level in noise_levels:\n                noise = torch.randn_like(mu) * noise_level\n                z_modified = mu + noise\n                fake_mel_norm = self.vae.decode(z_modified).cpu().numpy().squeeze()\n                fake_mel_norm = np.clip(fake_mel_norm, 0, 1)\n                fake_mel = fake_mel_norm * (mel_max - mel_min) + mel_min\n                deepfake_results.append((fake_mel, noise_level))\n                del noise, z_modified, fake_mel_norm\n                torch.cuda.empty_cache()\n\n            del input_tensor, mu, logvar\n            torch.cuda.empty_cache()\n\n        return deepfake_results\n\n    def evaluate_model(self, test_spectrograms, beta=0.1):\n        if self.vae is None:\n            print(\"Error: VAE model not trained for evaluation.\")\n            return None\n\n        self.vae.eval()\n        total_loss = 0\n        total_samples = 0\n        with torch.no_grad():\n            dataloader = DataLoader(AudioDataset(test_spectrograms), batch_size=1, shuffle=False, num_workers=0, pin_memory=True)\n            for batch_idx, batch in enumerate(dataloader):\n                batch_tensor = batch.to(self.device, non_blocking=True)\n                with amp.autocast(enabled=self.device.type == 'cuda'):\n                    recon_batch, mu, logvar = self.vae(batch_tensor)\n                    recon_loss, kl_loss = self.improved_vae_loss(recon_batch, batch_tensor, mu, logvar, beta=beta)\n                total_loss += (recon_loss + kl_loss).item() * len(batch)\n                total_samples += len(batch)\n                del batch_tensor, recon_batch, mu, logvar\n                torch.cuda.empty_cache()\n\n        if total_samples > 0:\n            avg_loss = total_loss / total_samples\n            print(f\"Evaluation Loss: {avg_loss:.4f}\")\n            return avg_loss\n        else:\n            print(\"No test samples available for evaluation.\")\n            return None\n\n    def compare_audio(self, original_mel, fake_mel):\n        min_height = min(original_mel.shape[0], fake_mel.shape[0])\n        min_width = min(original_mel.shape[1], fake_mel.shape[1])\n        original_mel_clipped = original_mel[:min_height, :min_width]\n        fake_mel_clipped = fake_mel[:min_height, :min_width]\n\n        orig_flat = original_mel_clipped.flatten()\n        fake_flat = fake_mel_clipped.flatten()\n\n        mse = mean_squared_error(orig_flat, fake_flat)\n        if np.all(orig_flat == orig_flat[0]) or np.all(fake_flat == fake_flat[0]):\n            cosine_sim = np.nan\n            correlation = np.nan\n        else:\n            cosine_sim = 1 - cosine(orig_flat, fake_flat)\n            correlation = np.corrcoef(orig_flat, fake_flat)[0, 1]\n\n        print(\"=== Audio Comparison Results ===\")\n        print(f\"Mean Squared Error: {mse:.4f}\")\n        print(f\"Cosine Similarity: {cosine_sim:.4f}\")\n        print(f\"Correlation: {correlation:.4f}\")\n        return {'mse': mse, 'cosine_similarity': cosine_sim, 'correlation': correlation}\n\n    def visualize_comparison(self, original_mel, fake_mel):\n            min_height = min(original_mel.shape[0], fake_mel.shape[0])\n            min_width = min(original_mel.shape[1], fake_mel.shape[1])\n            original_mel_clipped = original_mel[:min_height, :min_width]\n            fake_mel_clipped = fake_mel[:min_height, :min_width]\n    \n            fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n            im1 = axes[0].imshow(original_mel_clipped, aspect='auto', origin='lower', cmap='viridis')\n            axes[0].set_title('Original Mel Spectrogram')\n            axes[0].set_xlabel('Time')\n            axes[0].set_ylabel('Mel Frequency')\n            plt.colorbar(im1, ax=axes[0])\n            im2 = axes[1].imshow(fake_mel_clipped, aspect='auto', origin='lower', cmap='viridis')\n            axes[1].set_title('Generated Deepfake Mel Spectrogram')\n            axes[1].set_xlabel('Time')\n            axes[1].set_ylabel('Mel Frequency')\n            plt.colorbar(im2, ax=axes[1])\n            diff = np.abs(original_mel_clipped - fake_mel_clipped)\n            im3 = axes[2].imshow(diff, aspect='auto', origin='lower', cmap='hot')\n            axes[2].set_title('Absolute Difference')\n            axes[2].set_xlabel('Time')\n            axes[2].set_ylabel('Mel Frequency')\n            plt.colorbar(im3, ax=axes[2])\n            plt.tight_layout()\n            plt.close()\n\n    def save_audio(self, mel_spec, filename, sr=22050):\n        audio = self.processor.mel_spectrogram_to_audio(mel_spec)\n        sf.write(filename, audio, sr)\n        print(f\"Audio saved as: {filename}\")\n        \n    def generate_deepfake(self, original_mel_spec, noise_levels=[0.0, 0.1]):\n        if self.vae is None:\n            print(\"Error: VAE model not trained. Please train the model first.\")\n            return None\n\n        self.vae.eval()\n        deepfake_results = []\n\n        with torch.no_grad():\n            if original_mel_spec.shape != self.target_shape:\n                print(f\"Warning: Input mel shape {original_mel_spec.shape} does not match VAE target shape {self.target_shape}. Resizing.\")\n                original_mel_spec = np.resize(original_mel_spec, self.target_shape)\n\n            mel_min, mel_max = np.percentile(original_mel_spec, [1, 99])\n            if mel_max <= mel_min:\n                print(\"Error: Invalid mel spectrogram range for generation.\")\n                return original_mel_spec\n\n            mel_norm = np.clip((original_mel_spec - mel_min) / (mel_max - mel_min + 1e-8), 0, 1)\n            input_tensor = torch.FloatTensor(mel_norm[None, None, :, :]).to(self.device, non_blocking=True)\n\n            mu, logvar = self.vae.encode(input_tensor)\n            for noise_level in noise_levels:\n                noise = torch.randn_like(mu) * noise_level\n                z_modified = mu + noise\n                fake_mel_norm = self.vae.decode(z_modified).cpu().numpy().squeeze()\n                fake_mel_norm = np.clip(fake_mel_norm, 0, 1)\n                fake_mel = fake_mel_norm * (mel_max - mel_min) + mel_min\n                deepfake_results.append((fake_mel, noise_level))\n                del noise, z_modified, fake_mel_norm\n                torch.cuda.empty_cache()\n\n            del input_tensor, mu, logvar\n            torch.cuda.empty_cache()\n\n        return deepfake_results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Main Function","metadata":{}},{"cell_type":"code","source":"def get_image_files(directory):\n    image_files = []\n    valid_extensions = ('.png', '.jpg', '.jpeg')\n    if not os.path.exists(directory):\n        print(f\"Directory {directory} does not exist\")\n        return image_files\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith(valid_extensions):\n                file_path = os.path.join(root, file)\n                try:\n                    img = Image.open(file_path).convert('L')\n                    img_array = np.array(img, dtype=np.float32)\n                    image_files.append((file_path, img_array))\n                except Exception as e:\n                    print(f\"Error loading image {file_path}: {e}\")\n    return image_files\n\ndef main():\n    torch.cuda.empty_cache()\n    generator = AudioDeepfakeGenerator(height=128, width=128)\n    base_path = \"/kaggle/input/vae128-ctrsdd\"\n    train_real_path = os.path.join(base_path, \"spec_128\")\n    \n    train_files = get_image_files(train_real_path)\n    if not train_files:\n        print(\"No image files found in the training directory.\")\n        return\n\n    train_files, test_files = train_test_split(train_files, test_size=0.2, random_state=42)\n    val_files = test_files\n\n    print(f\"Found {len(train_files)} training files\")\n    print(f\"Found {len(val_files)} validation files\")\n    print(f\"Found {len(test_files)} testing files\")\n\n    print(\"\\nStep 1: Processing training mel spectrograms...\")\n    try:\n        train_spectrograms = generator.preprocess_mel_spectrograms(train_files)\n    except ValueError as e:\n        print(f\"Error: {e}\")\n        return\n\n    print(\"\\nStep 2: Processing validation mel spectrograms...\")\n    try:\n        val_spectrograms = generator.preprocess_mel_spectrograms(val_files)\n    except ValueError as e:\n        print(f\"Error: {e}\")\n        return\n\n    print(\"\\nStep 3: Processing testing mel spectrograms...\")\n    try:\n        test_spectrograms = generator.preprocess_mel_spectrograms(test_files)\n    except ValueError as e:\n        print(f\"Error: {e}\")\n        return\n\n    print(f\"Training samples: {len(train_spectrograms)}\")\n    print(f\"Validation samples: {len(val_spectrograms)}\")\n    print(f\"Testing samples: {len(test_spectrograms)}\")\n\n    print(\"\\nStep 4: Training convolutional VAE...\")\n    generator.train_vae(\n        mel_spectrograms=train_spectrograms,\n        epochs=250,\n        batch_size=32,\n        learning_rate=1e-4,\n        beta=0.1,\n        early_stop_patience=60,\n        accum_steps=2\n    )\n\n    print(\"\\nStep 5: Evaluating model on validation set...\")\n    generator.evaluate_model(val_spectrograms, beta=0.1)\n\n    print(\"\\nStep 6: Evaluating model on test set...\")\n    generator.evaluate_model(test_spectrograms, beta=0.1)\n\n    print(\"\\nStep 7: Preparing reference for deepfake generation...\")\n    for j in range(11):\n        reference_file, reference_mel = test_files[j]\n        if reference_mel.shape != generator.target_shape:\n            if reference_mel.shape[0] > generator.target_shape[0]:\n                reference_mel = reference_mel[:generator.target_shape[0], :]\n            elif reference_mel.shape[0] < self.target_shape[0]:\n                pad_height = generator.target_shape[0] - reference_mel.shape[0]\n                reference_mel = np.pad(reference_mel, ((0, pad_height), (0, 0)), 'constant', constant_values=reference_mel.min())\n    \n            if reference_mel.shape[1] > generator.target_shape[1]:\n                reference_mel = reference_mel[:, :generator.target_shape[1]]\n            elif reference_mel.shape[1] < generator.target_shape[1]:\n                pad_width = generator.target_shape[1] - reference_mel.shape[1]\n                reference_mel = np.pad(reference_mel, ((0, 0), (0, pad_width)), 'constant', constant_values=reference_mel.min())\n    \n        print(f\"Reference file: {os.path.basename(reference_file)}, shape: {reference_mel.shape}\")\n        generator.processor.plot_mel_spectrogram(reference_mel, \"Original Reference Mel Spectrogram\")\n    \n        print(\"\\nStep 8: Generating high-quality deepfakes...\")\n        noise_levels = [0.0]\n        deepfake_results = generator.generate_deepfake(reference_mel, noise_levels=noise_levels)\n    \n        for i, (fake_mel, noise_level) in enumerate(deepfake_results):\n            print(f\"Generated deepfake variation {i+1}/{len(deepfake_results)} (noise={noise_level})...\")\n            generator.save_audio(fake_mel, f\"high_quality_deepfake_{j+1}_noise_{noise_level}.wav\")\n            results = generator.compare_audio(reference_mel, fake_mel)\n            print(f\"  MSE: {results['mse']:.4f}, Cosine Sim: {results['cosine_similarity']:.4f}\")\n    \n        best_idx = 0\n        best_fake_mel, best_noise = deepfake_results[best_idx]\n        generator.processor.plot_mel_spectrogram(best_fake_mel, \"fake Reference Mel Spectrogram\")\n        print(f\"\\nStep 9: Detailed analysis of best deepfake (noise={best_noise})...\")\n        detailed_results = generator.compare_audio(reference_mel, best_fake_mel)\n    \n        print(\"\\nStep 10: Visualizing results...\")\n        generator.visualize_comparison(reference_mel, best_fake_mel)\n        generator.save_audio(reference_mel, \"original_reference_improved.wav\")\n\n    print(\"\\nImproved deepfake generation completed!\")\n    print(f\"Generated {len(deepfake_results)} high-quality variations\")\n    print(\"Files saved:\")\n    print(\"- original_reference_improved.wav\")\n    for i, (_, noise) in enumerate(deepfake_results):\n        print(f\"- high_quality_deepfake_{i+1}_noise_{noise}.wav\")\n\n    print(f\"\\nPerformance Summary:\")\n    print(f\"Best deepfake metrics:\")\n    print(f\"  MSE: {detailed_results['mse']:.6f}\")\n    print(f\"  Cosine Similarity: {detailed_results['cosine_similarity']:.6f}\")\n    print(f\"  Correlation: {detailed_results['correlation']:.6f}\")\n\n    torch.cuda.empty_cache()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}